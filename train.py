# TensorFlow ã¨ tf.keras ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import tensorflow as tf
from tensorflow import keras

# ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import numpy as np
import matplotlib.pyplot as plt

import os
import pathlib
import time
import datetime
import glob
import random
import cv2

from PIL import Image
from IPython import display

def load(image_path):
    # å…¥åŠ›ç”»åƒã®ãƒ‘ã‚¹ã€æ­£è§£ç”»åƒã®ãƒ‘ã‚¹
    input_image_path, real_image_path = image_path
    
    # å…¥åŠ›ç”»åƒã®èª­ã¿è¾¼ã¿
    input_image = tf.io.read_file(input_image_path) # å…¥åŠ›ç”»åƒã®å–å¾—
    # JPEGã®ç”»åƒã‚’uint8å½¢å¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    input_image = tf.io.decode_jpeg(input_image, channels=CHANNEL) 
    # æ­£è§£ç”»åƒã®èª­ã¿è¾¼ã¿
    real_image = tf.io.read_file(real_image_path)
    # JPEGã®ç”»åƒã‚’uint8å½¢å¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    real_image = tf.io.decode_jpeg(real_image, channels=CHANNEL)

    # uint8ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’å­¦ç¿’ã—ã‚„ã™ã„ã‚ˆã†ã«float32ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    input_image = tf.cast(input_image, tf.float32)
    real_image = tf.cast(real_image, tf.float32)

    return input_image, real_image

# å­¦ç¿’ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
def read_bd_rm(dataset, batch_size=1, size=256):
        train_image_dataset = dataset

        batch_input = [] # å…¥åŠ›ç”»åƒã®ãƒãƒƒãƒ
        batch_real = [] # æ­£è§£ç”»åƒã®ãƒãƒƒãƒ

        for pd in train_image_dataset:
            input = pd[0] # å®Ÿæ…‹ã¯å…¥åŠ›ç”»åƒã®ãƒ‘ã‚¹
            real = pd[1] # å®Ÿæ…‹ã¯æ­£è§£ç”»åƒã®ãƒ‘ã‚¹
            
            # ãƒ¢ãƒã‚¯ãƒ­ç”»åƒã®å ´åˆã€ãƒ¢ãƒã‚¯ãƒ­ç”»åƒã¨ã—ã¦ç”»åƒã‚’å–å¾—
            if CHANNEL == 1:
                input = Image.open(input).convert("L")
                input = input.resize((256, 256))
                real = Image.open(real).convert("L")
                real = real.resize((256, 256))
            else:
                input = Image.open(input)
                input = input.resize((256, 256))
                real = Image.open(real)
                real = real.resize((256, 256))
            # å…¥åŠ›ç”»åƒã®å½¢å¼ï¼ˆã‚·ã‚§ã‚¤ãƒ—ï¼‰ã‚’tensorflowã€kerasã§å­¦ç¿’ã™ã‚‹ãŸã‚ã«å¤‰æ›
            input = np.reshape(input, [1, size, size, CHANNEL])
            # æ­£è§£ç”»åƒã®å½¢å¼ï¼ˆã‚·ã‚§ã‚¤ãƒ—ï¼‰ã‚’tensorflowã€kerasã§å­¦ç¿’ã™ã‚‹ãŸã‚ã«å¤‰æ›
            real = np.reshape(real, [1, size, size, CHANNEL])
            # å…¥åŠ›ç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            input = tf.cast(tf.convert_to_tensor(np.asarray(input)), dtype=tf.float32) / 255.
            # æ­£è§£ç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            real = tf.cast(tf.convert_to_tensor(np.asarray(real)), dtype=tf.float32) / 255.

            # å…¥åŠ›ç”»åƒã®ãƒãƒƒãƒ
            batch_input += [input]
            # æ­£è§£ç”»åƒã®ãƒãƒƒãƒ
            batch_real += [real]
            
            # ä»Šå›ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯1ãªã®ã§ã€å˜ç´”ã«ãƒšã‚¢ã®ç”»åƒã‚’è¿”ã—ã¦ã„ã‚‹ã ã‘
            if len(batch_input) ==  batch_size:
                batch_input = tf.concat(batch_input, axis=0)
                batch_real = tf.concat(batch_real, axis=0)

                yield {'input': batch_input, 'real': batch_real}
                batch_input = []
                batch_real = []

# å­¦ç¿’ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
def read_single_image(dataset, batch_size=1, size=256):
        train_image_dataset = dataset

        batch_input = [] # å…¥åŠ›ç”»åƒã®ãƒãƒƒãƒ

        for pd in train_image_dataset:
            input = pd[0] # å®Ÿæ…‹ã¯å…¥åŠ›ç”»åƒã®ãƒ‘ã‚¹
            
            # ãƒ¢ãƒã‚¯ãƒ­ç”»åƒã®å ´åˆã€ãƒ¢ãƒã‚¯ãƒ­ç”»åƒã¨ã—ã¦ç”»åƒã‚’å–å¾—
            if CHANNEL == 1:
                input = Image.open(input).convert("L")
                input = input.resize((256, 256))
            else:
                input = Image.open(input)
                input = input.resize((256, 256))
            # å…¥åŠ›ç”»åƒã®å½¢å¼ï¼ˆã‚·ã‚§ã‚¤ãƒ—ï¼‰ã‚’tensorflowã€kerasã§å­¦ç¿’ã™ã‚‹ãŸã‚ã«å¤‰æ›
            input = np.reshape(input, [1, size, size, CHANNEL])
            # å…¥åŠ›ç”»åƒã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            input = tf.cast(tf.convert_to_tensor(np.asarray(input)), dtype=tf.float32) / 255.

            # å…¥åŠ›ç”»åƒã®ãƒãƒƒãƒ
            batch_input += [input]
            
            # ä»Šå›ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯1ãªã®ã§ã€å˜ç´”ã«ãƒšã‚¢ã®ç”»åƒã‚’è¿”ã—ã¦ã„ã‚‹ã ã‘
            if len(batch_input) ==  batch_size:
                batch_input = tf.concat(batch_input, axis=0)

                yield {'input': batch_input}
                batch_input = []

def train_data_loader(batch_size=1):
    # ç”»åƒã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆã™ã‚‹
    input_paths = sorted(glob.glob(os.path.join(str(PATH), '*.png'))) # å…¥åŠ›ç”»åƒã®ãƒ‘ã‚¹ã®é…åˆ—
    real_paths = sorted(glob.glob(os.path.join(str(PATH), '*.png'))) # æ­£è§£ç”»åƒã®ãƒ‘ã‚¹ã®é…åˆ—
    
    # ãƒšã‚¢ã®ç”»åƒã®ãƒ‘ã‚¹ã‚’è£œå®Œã™ã‚‹ãƒªã‚¹ãƒˆ
    records = []
    
    # ç”»åƒã®ãƒ‘ã‚¹ã®ãƒšã‚¢ã‚’ä½œæˆ
    # æ­£è§£ç”»åƒã®ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒãŒä¸€æšã—ã‹ãªã„ãŸã‚ã€QRã‚³ãƒ¼ãƒ‰ã®æ™‚ã ã‘ã“ã£ã¡ã‚’ä½¿ç”¨
    for input in input_paths:
        records += [[input, real_paths[0]]]

    # QRã‚³ãƒ¼ãƒ‰ä»¥å¤–ï¼ˆæ­£è§£ç”»åƒã¨å…¥åŠ›ç”»åƒã®æšæ•°ãŒåŒã˜å ´åˆï¼‰ã®å ´åˆã¯ä¸Šã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã€ã“ã£ã¡ã‚’ä½¿ç”¨
    # for input, real in zip(input_paths, real_paths):
    #     records += [[input, real]]

    return read_bd_rm(records, batch_size=batch_size) # ç”»åƒã®ãƒšã‚¢ã®ãƒ‘ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«æ¸¡ã™

def test_data_loader(batch_size=1):
    input_paths = sorted(glob.glob(os.path.join(str(PATH) + "/test/" , '*.jpg'))) # QRã‚³ãƒ¼ãƒ‰ç”»
    # input_paths = sorted(glob.glob(os.path.join(str(PATH) + "/test/" , 'tr*.jpg'))) # æ•°å­—ç”»åƒ
    records = []
    for input in input_paths:
        records += [[input]]

    return read_single_image(records, batch_size=batch_size)

# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã£ã¦ã„ã‚‹éƒ¨åˆ†
def train_step(input_image, target, step, epoch):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ç”»åƒã‚’å…¥åŠ›ã—ã€ç”Ÿæˆç”»åƒã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†
        gen_output = generator(input_image, training=True)

        # å…¥åŠ›ç”»åƒã¨æ­£è§£ç”»åƒã‚’å…¥åŠ›ã—ã€ãƒ­ã‚¹è¨ˆç®—ã®ãŸã‚ã®ç”»åƒã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†
        disc_real_output = discriminator([input_image, target], training=True)
        # å…¥åŠ›ç”»åƒã¨ç”Ÿæˆç”»åƒã‚’å…¥åŠ›ã—ã€åˆ¤åˆ¥è¨ˆç®—ã®ãŸã‚ã®ç”»åƒã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†
        disc_generated_output = discriminator([input_image, gen_output], training=True)
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¹è¨ˆç®—
        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
        # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¹è¨ˆç®—
        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®å‹¾é…
    generator_gradients = gen_tape.gradient(gen_total_loss,
                                            generator.trainable_variables)
    # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ã®å‹¾é…
    discriminator_gradients = disc_tape.gradient(disc_loss,
                                                discriminator.trainable_variables)

    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®å‹¾é…æ›´æ–°
    generator_optimizer.apply_gradients(zip(generator_gradients,
                                            generator.trainable_variables))
    # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ã®å‹¾é…æ›´æ–°
    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                                discriminator.trainable_variables))
    
    if step % 10 == 0:
        # ãƒ­ã‚¹ã®å‡ºåŠ›
        print("epoch: {} step: {} gen_total_loss: {}".format(epoch, step, str(gen_total_loss.numpy())))
        print("epoch: {} step: {} gen_gan_loss: {}".format(epoch, step, str(gen_gan_loss.numpy())))
        print("epoch: {} step: {} disc_loss: {}".format(epoch, step, str(disc_loss.numpy())))

        # tensorboardã§ã®å­¦ç¿’çµŒéã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®å‡¦ç†
        with summary_writer.as_default():
            tf.summary.scalar('gen_total_loss', gen_total_loss, step)
            tf.summary.scalar('gen_gan_loss', gen_gan_loss, step)
            tf.summary.scalar('gen_l1_loss', gen_l1_loss, step)
            tf.summary.scalar('disc_loss', disc_loss, step)


def fit():
    for epoch in range(1, EPOCH+1):
        step = 1
        # å­¦ç¿’ã«ä½¿ã†ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®å‘¼ã³å‡ºã—ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ˆã†ãªã‚‚ã®ï¼‰
        train_generator = train_data_loader(batch_size=8)

        start = time.time()
        for train_loader_dict in train_generator:
            input_image = train_loader_dict['input'] # å…¥åŠ›ç”»åƒã‚’å–å¾—ã™ã‚‹
            target = train_loader_dict['real'] # æ­£è§£ç”»åƒã‚’å–å¾—ã™ã‚‹
            # å­¦ç¿’ã‚’1ã‚¹ãƒ†ãƒƒãƒ—é€²ã‚ã‚‹
            train_step(input_image, target, step, epoch)
            step = step + 1

            if step % 10 == 0:
                # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚»ãƒ«ã®å‡ºåŠ›ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹
                display.clear_output(wait=True)
                # # ç”»åƒã‚’å­¦ç¿’çµŒéä¸­ã«ä¿å­˜ã™ã‚‹
                # generate_images(generator, example_input, target)
        start = time.time()
        
        # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹
        if epoch % 10 == 0:
            checkpoint_path = "checkpoints/cp-" + str(epoch) + ".h5"
            generator.save_weights(checkpoint_path)

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å®šç¾©ã«ä½¿ç”¨
def downsample(filters, kernel_size, strides=2, dropout=0.5, max_pool=True, batch_norm=True):
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()
    result.add(
        # ç•³ã¿è¾¼ã¿å±¤ã®è¿½åŠ 
        tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same',
                                kernel_initializer=initializer, use_bias=False))
    # max poolingã‚’è¡Œã†å ´åˆ
    if max_pool:
        result.add(tf.keras.layers.MaxPool2D(pool_size=(1, 1), strides=None, padding='same'))

    # ãƒãƒƒãƒãƒãƒ«ãƒ ã‚’è¡Œã†å ´åˆ
    if batch_norm:
        result.add(tf.keras.layers.BatchNormalization())
    
    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã†å ´åˆ
    if dropout != None:
        result.add(tf.keras.layers.Dropout(dropout))

    result.add(tf.keras.layers.LeakyReLU())
    return result


# ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®å®šç¾©ã«ä½¿ç”¨
def upsample(filters, size,  dropout=0.5, max_pool=True, batch_norm=True):
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()
    result.add(
        # ç•³ã¿è¾¼ã¿å±¤ã®è¿½åŠ ï¼ˆã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã«ä½¿ç”¨ï¼‰
        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                        padding='same',
                                        kernel_initializer=initializer,
                                        use_bias=False)
    )
    
    # max poolingã‚’è¡Œã†å ´åˆ
    if max_pool:
        result.add(tf.keras.layers.MaxPool2D(pool_size=(1, 1), strides=None, padding='same'))

    # ãƒãƒƒãƒãƒãƒ«ãƒ ã‚’è¡Œã†å ´åˆ
    if batch_norm:
        result.add(tf.keras.layers.BatchNormalization())

    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã†å ´åˆ
    if dropout != None:
        result.add(tf.keras.layers.Dropout(dropout))
    result.add(tf.keras.layers.ReLU())

    return result

# ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®šç¾©
def Generator(image_shape):
    initializer = tf.random_normal_initializer(0., 0.02)
    # å…¥åŠ›ç”»åƒ
    input_image = keras.layers.Input(shape=image_shape, name='input_image')
    x = input_image

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å®šç¾©
    enc1 = downsample(n_E1, kernel_size_E1, stride_E1, DropOut_E1, MaxPooling_E1, BatchNorm_E1)(x) # æ­£ä½“ã¯å˜ç´”ãªç•³ã¿è¾¼ã¿å±¤
    enc2 = downsample(n_E2, kernel_size_E2 ,stride_E2, DropOut_E2, MaxPooling_E2, BatchNorm_E2)(enc1)
    enc3 = downsample(n_E3, kernel_size_E3, stride_E3, DropOut_E3, MaxPooling_E3, BatchNorm_E3)(enc2)
    enc4 = downsample(n_E4, kernel_size_E4, stride_E4, DropOut_E4, MaxPooling_E4, BatchNorm_E4)(enc3)
    enc5 = downsample(n_E5, kernel_size_E5 ,stride_E5, DropOut_E5, MaxPooling_E5, BatchNorm_E5)(enc4)
    enc6 = downsample(n_E6, kernel_size_E6 ,stride_E6, DropOut_E6, MaxPooling_E6, BatchNorm_E6)(enc5)
    enc7 = downsample(n_E7, kernel_size_E7 ,stride_E7, DropOut_E7, MaxPooling_E7, BatchNorm_E7)(enc6)
    enc8 = downsample(n_E8, kernel_size_E8, stride_E8, DropOut_E8, MaxPooling_E8, BatchNorm_E8)(enc7)

    # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®å®šç¾©
    dec1 = upsample(n_E7, kernel_size_E7, DropOut_E7, MaxPooling_E7, BatchNorm_E7) # æ­£ä½“ã¯å˜ç´”ãªç•³ã¿è¾¼ã¿å±¤
    dec2 = upsample(n_E6, kernel_size_E6, DropOut_E6, MaxPooling_E6, BatchNorm_E6)
    dec3 = upsample(n_E5, kernel_size_E5, DropOut_E5, MaxPooling_E5, BatchNorm_E5)
    dec4 = upsample(n_E4, kernel_size_E4, DropOut_E4, MaxPooling_E4, BatchNorm_E4)
    dec5 = upsample(n_E3, kernel_size_E3, DropOut_E3, MaxPooling_E3, BatchNorm_E3)
    dec6 = upsample(n_E2, kernel_size_E2, DropOut_E2, MaxPooling_E2, BatchNorm_E2)
    dec7 = upsample(n_E1, kernel_size_E1, DropOut_E1, MaxPooling_E1, BatchNorm_E1)
    

    # ç”»åƒã‚’æ‹¡å¤§ã™ã‚‹å ´åˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    # zoom = upsample(CHANNEL, 4)

    # ãƒã‚¤ãƒ‘ã‚¹ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ç‰¹å¾´é‡
    enc_value_list = [enc7, enc6, enc5, enc4, enc3, enc2, enc1]

    # ãƒã‚¤ãƒ‘ã‚¹ã™ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ç‰¹å¾´é‡
    dec_value_list = [dec1, dec2, dec3, dec4, dec5, dec6, dec7]

    # ãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã†ã‹åˆ¤å®šã™ã‚‹ãƒ•ãƒ©ã‚°
    bipass_list = [Bipass_7, Bipass_6, Bipass_5, Bipass_4, Bipass_3, Bipass_2, Bipass_1]

    # ãƒã‚¤ãƒ‘ã‚¹å‡¦ç†ã®ãŸã‚ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æœ€çµ‚å‡ºåŠ›ã‚’xã¨ã™ã‚‹
    x = enc8    

    # ãƒã‚¤ãƒ‘ã‚¹å‡¦ç†ã‚’è¡Œã£ã¦ã„ã‚‹éƒ¨åˆ†
    for dec, enc, bipass in zip(dec_value_list, enc_value_list, bipass_list):
        x = dec(x)
        # ãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã†ã‹ã‚’åˆ¤å®šã™ã‚‹
        if bipass:
            x = tf.keras.layers.Concatenate()([x, enc]) # ãƒã‚¤ãƒ‘ã‚¹ã—ã¦ã„ã‚‹è¡Œ

    # ç”»åƒã‚’æ‹¡å¤§ã™ã‚‹å ´åˆã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™
    # if expantion:
    #     x = zoom(x)

    # å‡ºåŠ›ã®ãƒãƒ£ãƒãƒ«æ•°ã€ã‚«ãƒ©ãƒ¼ã ã¨3ã€ãƒ¢ãƒã‚¯ãƒ­ã ã¨1
    OUTPUT_CHANNELS = CHANNEL

    # å±¤ã®ä¹±æ•°ã‚’åˆæœŸåŒ–ã™ã‚‹ã‚‚ã®
    initializer = tf.random_normal_initializer(0., 0.02)

    # æœ€çµ‚å‡ºåŠ›å±¤
    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                            strides=2,
                                            padding='same',
                                            kernel_initializer=initializer,
                                            activation='tanh')
    x = last(x)

    return tf.keras.Model(inputs=input_image, outputs=x)


# ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®ãƒ­ã‚¹ã®è¨ˆç®—éƒ¨åˆ†
def generator_loss(disc_generated_output, gen_output, target):
  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)

  # Mean absolute error
  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))

  total_gen_loss = gan_loss + (LAMBDA * l1_loss)

  return total_gen_loss, gan_loss, l1_loss

# ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®šç¾©
def Discriminator(image_shape):
    initializer = tf.random_normal_initializer(0., 0.02)

    input_image = tf.keras.layers.Input(image_shape, name='input_image')
    target_image = tf.keras.layers.Input(image_shape, name='target_image')

    x = tf.keras.layers.concatenate([input_image, target_image])  # (batch_size, 256, 256, channels*2)

    # å…¥åŠ›ç”»åƒã®ã‚µã‚¤ã‚ºãŒ512x512ã®å ´åˆã«å±¤ã‚’è¿½åŠ ã™ã‚‹
    if image_shape == (512, 512, 1):
        x = downsample(CHANNEL, 4)(x)
    down1 = downsample(n_E1, kernel_size_E1, 1, DropOut_E1, MaxPooling_E1, BatchNorm_E1)(x)
    down2 = downsample(n_E2, kernel_size_E2, 1, DropOut_E2, MaxPooling_E2, BatchNorm_E2)(x)
    down3 = downsample(n_E3, kernel_size_E3, stride_E3, DropOut_E3, MaxPooling_E3, BatchNorm_E3)(down2)
    down4 = downsample(n_E4, kernel_size_E4, stride_E4, DropOut_E4, MaxPooling_E4, BatchNorm_E4)(down3)
    down5 = downsample(n_E5, kernel_size_E5, stride_E5, DropOut_E5, MaxPooling_E5, BatchNorm_E5)(down4)
    down6 = downsample(n_E6, kernel_size_E6, stride_E6, DropOut_E6, MaxPooling_E6, BatchNorm_E6)(down5)
    last = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same',
                                     kernel_initializer=initializer)  # (batch_size, 30, 30, 1)

    x = last(down6)
    return tf.keras.Model(inputs=[input_image, target_image], outputs=x)


def discriminator_loss(disc_real_output, disc_generated_output):
    # å…¥åŠ›ç”»åƒã¨æ­£è§£ç”»åƒã®ãƒ­ã‚¹â‘ 
    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)
    # å…¥åŠ›ç”»åƒã®ç”Ÿæˆç”»åƒã®ãƒ­ã‚¹â‘¡
    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)
    # â‘ ã¨â‘¡ã¨ã®ãƒˆãƒ¼ã‚¿ãƒ«ãƒ­ã‚¹
    total_disc_loss = real_loss + generated_loss

    return total_disc_loss

# å­¦ç¿’ã«ä½¿ã†ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
generator_optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) 
discriminator_optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)

def generate_images(model, test_input, tar):
  prediction = model(test_input, training=True)
  plt.figure(figsize=(15, 15))

  display_list = [test_input[0], tar[0], prediction[0]]
  title = ['Input Image', 'Ground Truth', 'Predicted Image']

  for i in range(3):
    plt.subplot(1, 3, i+1)
    plt.title(title[i])
    # Getting the pixel values in the [0, 1] range to plot.
    if CHANNEL == 1:
        plt.imshow(display_list[i].numpy().flatten().reshape(256, 256) * 0.5 + 0.5, cmap='gray') # ãƒ¢ãƒã‚¯ãƒ­ç”»åƒã®å ´åˆ
    else:
        plt.imshow(display_list[i]) # ã‚«ãƒ©ãƒ¼ç”»åƒã®å ´åˆ
    plt.axis('off')
  plt.show()



train_images = glob.glob('bottle/train/good/*')
train = []
for im in train_images:
    image = cv2.imread(im)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (256, 256))
    train.append(image)

train = np.array(train)
train = train.astype('float32') /255.


PATH = 'bottle/train/good'
# ã‚«ãƒ©ãƒ¼ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’æŒ‡å®šã€ã‚«ãƒ©ãƒ¼:3ã€ãƒ¢ãƒã‚¯ãƒ­1ã‚’æŒ‡å®š
CHANNEL = 1

# å…¥åŠ›ç”»åƒã‚µã‚¤ã‚º
G_input_dim = (256, 256, CHANNEL) 
EPOCH = 20

# ç¬¬ï¼‘å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
n_E1 = 32 # ãƒãƒ£ãƒãƒ«æ•°
m_E1 = 128 # ç”»ç´ æ•°
stride_E1 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E1 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E1 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E1 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
BatchNorm_E1 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E1 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_1 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ 

# ç¬¬ï¼’å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E2 = 64 # ãƒãƒ£ãƒãƒ«æ•°
m_E2 = 64 # ç”»ç´ æ•°
stride_E2 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E2 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E2 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E2 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E2 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E2 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_2 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ 

# ç¬¬3å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E3 = 128 # ãƒãƒ£ãƒãƒ«æ•°
m_E3 = 128 # ç”»ç´ æ•°
stride_E3 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E3 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E3 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E3 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E3 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E3 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_3 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ 

# ç¬¬4å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E4 = 256 # ãƒãƒ£ãƒãƒ«æ•°
m_E4 = 256 # ç”»ç´ æ•°
stride_E4 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E4 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E4 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E4 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E4 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E4 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_4 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ 

# ç¬¬5å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E5 = 512 # ãƒãƒ£ãƒãƒ«æ•°
m_E5 = 512 # ç”»ç´ æ•°
stride_E5 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E5 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E5 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E5 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E5 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E5 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_5 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰

# ç¬¬6å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E6 = 512 # ãƒãƒ£ãƒãƒ«æ•°
m_E6 = 512 # ç”»ç´ æ•°
stride_E6 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E6 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E6 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E6 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E6 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E6 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_6 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰

# ç¬¬7å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E7 = 512 # ãƒãƒ£ãƒãƒ«æ•°
m_E7 = 512 # ç”»ç´ æ•°
stride_E7 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E7 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E7 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E7 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E7 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E7 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
Bipass_7 = True # ãƒã‚¤ãƒ‘ã‚¹ã®è¨­å®šï¼ˆãƒã‚¤ãƒ‘ã‚¹ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰


# ç¬¬8å±¤ç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
n_E8 = 512 # ãƒãƒ£ãƒãƒ«æ•°
m_E8 = 512 # ç”»ç´ æ•°
stride_E8 = 2 # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®ã‚µã‚¤ã‚º
kernel_size_E8 = 4 # ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
MaxPooling_E8 = True # MaxPoolingã®è¨­å®šï¼ˆMaxPoolingã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰ğŸ‡²
ActivationFunc_E8 = "Leaky_ReLu" # æ´»æ€§åŒ–é–¢æ•°
alfa = 0.2
BatchNorm_E8 = True # ã€€ãƒãƒƒãƒæ­£è¦åŒ–ã®è¨­å®šï¼ˆæ­£è¦åŒ–ã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰
DropOut_E8 = 0.5 # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®è¨­å®šï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è¡Œã‚ãªã„å ´åˆã¯Noneã«ã™ã‚‹ï¼‰

# ç”»åƒæ‹¡å¤§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
expantion = True # ç”»åƒã‚’conv2dã§ï¼’å€ã«æ‹¡å¤§ã™ã‚‹

log_dir="logs/"

summary_writer = tf.summary.create_file_writer(
  log_dir + "fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

generator = Generator(G_input_dim)
# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)

LAMBDA = 100

loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
discriminator = Discriminator(G_input_dim)

fit()